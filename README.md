# Projeto: An√°lise de Dados de Transporte P√∫blico

## Equipe

| Nome Completo | RA | Fun√ß√£o |
|--------------|------|----------------|
| Geovana Neuberger Sorg | 22001825 | Membro da equipe |
| Fernando Pagliarini Furlanetto | 22000293 | Membro da equipe |
| Gustavo Henrique Tomaz da Silva | 22001161 | Scrum Master (SM) |
| Lucas de Oliveira Barreiro | 22000100 | Product Owner (PO) |
| Diogo Alves Daloca | 24001784 | Membro da equipe |

## Descri√ß√£o do Projeto

Este projeto tem como objetivo analisar grandes volumes de dados sobre o transporte p√∫blico de uma cidade, utilizando t√©cnicas de Big Data para identificar padr√µes de uso, hor√°rios de pico, efici√™ncia das rotas e poss√≠veis melhorias no sistema.

Utilizando Python, R, Postgree e Docker, processaremos de dados simulado para criar visualiza√ß√µes e relat√≥rios que possam auxiliar na tomada de decis√µes.

## Principais Funcionalidades

### Coleta de Dados:
- Dataset simulado

### Armazenamento e Processamento:
- Utiliza√ß√£o do Postgree para armazenar grandes volumes de dados n√£o estruturados.
- Tratamento e an√°lise de dados com Python e R.
- Implementa√ß√£o de pipelines automatizados com Ansible e Docker.
- Aplica√ß√£o de t√©cnicas de Big Data para identificar padr√µes.

### Visualiza√ß√£o e Relat√≥rios:
- Gr√°ficos interativos para demonstrar hor√°rios de pico e efici√™ncia das rotas.
- Identifica√ß√£o de pontos de congestionamento e atrasos recorrentes.
- Sugest√µes de otimiza√ß√£o com base nos dados coletados.

## Fun√ß√µes e Responsabilidades

### Scrum Master (SM) - Gustavo Henrique Tomaz da Silva
- L√≠der e facilitador do grupo
- Atribui√ß√£o de pap√©is e responsabilidades
- Defini√ß√£o de datas para reuni√µes internas
- Manuten√ß√£o do reposit√≥rio GitHub junto com o PO

### Product Owner (PO) - Lucas de Oliveira Barreiro
- Cria√ß√£o e manuten√ß√£o do cronograma de entregas (Trello, Jira, Notion, GitHub Projects)
- Acompanhamento do progresso do time
- Manuten√ß√£o do reposit√≥rio GitHub junto com o SM

## Tecnologias Utilizadas

### Linguagens de Programa√ß√£o:
- Python
- R

### Banco de Dados:
- PostgreSQL

### Ferramentas de Big Data:
- Pandas
- Apache Spark
- Flask

### Ferramentas de Gerenciamento e Automatiza√ß√£o:
- Jekins
- Docker

### Visualiza√ß√£o de Dados:
- Matplotlib
- Seaborn
- ggplot2

## üöÄ Como Instalar e Executar o Projeto

Este projeto utiliza **Docker** e **Docker Compose** para orquestrar diversos servi√ßos de forma consistente.

### ‚úÖ Pr√©-requisitos

Antes de come√ßar, instale:

- [Docker Engine](https://docs.docker.com/engine/install/)
- [Docker Compose](https://docs.docker.com/compose/install/)
- [Git](https://git-scm.com/)
- Editor de c√≥digo (VS Code, Sublime, etc.)
- Navegador Web (Chrome, Firefox, etc.)

---

### üìÅ 1. Clonar o Reposit√≥rio

```bash
git clone https://github.com/gustavohenriqueT/Projeto-PI.git
cd Projeto-PI
```

---

### üîê 2. Vari√°veis de Ambiente (Opcional)

Se necess√°rio, crie um arquivo `.env` na raiz do projeto para configurar vari√°veis sens√≠veis. Neste projeto, credenciais padr√£o est√£o no `docker-compose.yml` apenas para desenvolvimento.

---

### üèóÔ∏è 3. Construir e Iniciar os Servi√ßos

Execute na raiz do projeto:

```bash
docker-compose up --build -d
```

- `--build`: Reconstr√≥i as imagens.
- `-d`: Executa os servi√ßos em segundo plano.

---

### üåê 4. Acessar os Servi√ßos

| Servi√ßo                  | URL/Info                                      |
|--------------------------|-----------------------------------------------|
| **Dashboard Dash**       | [http://localhost:5000](http://localhost:5000) |
| **Spark Master UI**      | [http://localhost:8081](http://localhost:8081) |
| **PostgreSQL**           | `localhost:5432` (DB: `transport_db`, User: `admin`, Pass: `password`) |
| **Jenkins**              | [http://localhost:18080](http://localhost:18080) |
| **Grafana**              | [http://localhost:3000](http://localhost:3000) <br> Login: `admin` / `admin` |

---

### üìÑ 5. Ver Logs dos Cont√™ineres

Logs de um servi√ßo espec√≠fico:

```bash
docker-compose logs -f <nome_do_servico>
```

Logs de todos os servi√ßos:

```bash
docker-compose logs -f
```

---

### üõë 6. Parar os Servi√ßos

```bash
docker-compose down
```

Remover volumes (apaga dados persistentes):

```bash
docker-compose down -v
```

---

### üß† Estrutura do Pipeline de Dados

1. `postgres` inicia.
2. `data-generator` gera `transport_data.csv`.
3. `r-processing` processa e gera `transport_dataL.csv`.
4. Paralelamente:
   - `data-warehouse` popula o PostgreSQL.
   - `hadoop-simulations` roda simula√ß√µes.
   - `clustering-analysis` realiza clusteriza√ß√µes.
5. `spark-submit-job` executa an√°lises com Spark.
6. `web-dashboard` exibe resultados.
7. `jenkins` e `grafana` rodam como servi√ßos independentes.

---

